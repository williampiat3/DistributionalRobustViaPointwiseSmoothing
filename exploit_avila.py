import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset, Dataset
import torch.optim as optim
from ruszczynski import Ruszczynski


import numpy as np
import matplotlib.pyplot as plt

from scipy.stats import gennorm

import torchvision
import torchvision.datasets
import torchvision.transforms as transforms
import torch.distributions as distrib
from modified_attacks import LinfPGDAttackNew


# from Algorithms import continuous_ga

import math
from tqdm import tqdm
from scipy.integrate import dblquad
import copy
import pickle 
import pandas as pd

#function for assessing lipschitz constant of the network

from virmaux import run_lipSDP,run_lipschitz_algorithm
from sklearn import preprocessing

from copy import deepcopy
from pointwise_avila import GeneralizedRelu,MLPModel,\
	sample_sphere,build_adversarial_noise,build_l2_noise,sample_lp_ball,sample_gen_gaussian,\
	hinge_loss,noise_randomly,noise_sphere_randomly
	
def deactivate(model):
	for p in model.parameters():
		p.requires_grad=False
	


class MeanCounter():
	"""
	Lightweight class for simple mean computations this doesn't work for standart deviation
	"""
	def __init__(self,shape=1):
		self._mean = np.zeros(shape)
		self.counter =0

	def append(self,value):
		self.counter+=1
		self._mean += (value-self._mean)/self.counter

	@property
	def mean(self):
		return self._mean.squeeze()



#function to compute the accuracy of the model on a test set
def computing_accuracy(model,x,y,batch_size):
	model.eval()
	eps=0.
	size = x.shape[0]
	count=0
	if batch_size < x.shape[0]:
		with torch.no_grad():
			for i in range(0,x.shape[0],batch_size):
				count += batch_size
				output = model(x[i:i+batch_size])
				eps+= torch.sum(torch.argmax(output,dim=1)==y[i:i+batch_size]).item()
			return eps/count
	else:
		output = model(x)
		return torch.mean((torch.argmax(output,dim=1)==y).float()).item()

		
def compute_robust_accuracy(model,x,y,batch_size,ball_points_number,radius,p=2,normalisation_index=-1,device=None,dtype=None):
	"""
	Function to compute the robust and the mean accuracy of the points
	"""
	if ball_points_number>=100:
		# Using DataParallel for inference
		model = nn.DataParallel(model)

	model.eval()
	eps=0.
	eps2 = 0.
	size = x.shape[0]
	count=0
	if batch_size < x.shape[0]:
		with torch.no_grad():
			for i in range(0,x.shape[0],batch_size):
				input_value=x[i:i+batch_size]
				shape_batch = input_value.shape[:normalisation_index]
				shape_object = input_value.shape[normalisation_index:]
				norm_length = np.prod(shape_object)
				#the forward is alittle bit more complex than for linear models as we can't process batched datas in the convolutionnal layers
				# thus we nedd the merge the batch dimension and the sampling dimension before processing
				noise = sample_lp_ball(*(ball_points_number,*shape_batch,norm_length),p=p,device=device,dtype=dtype)
				perturbated_data = (input_value.view(*shape_batch,norm_length)+ noise*radius).reshape(*(ball_points_number*np.prod(shape_batch),*shape_object))
				output = model(perturbated_data)
				output = output.view((ball_points_number,shape_batch[0],*output.shape[1:]))
				y_broadcasted = y[i:i+batch_size].unsqueeze(0).expand(ball_points_number,*y[i:i+batch_size].shape)



				count += batch_size
				predicted_labels = (torch.argmax(output,dim=2)==y_broadcasted).long()
				eps+= torch.sum(torch.min(predicted_labels,dim=0)[0].float()).item()
				eps2 += torch.sum(torch.mean(predicted_labels.float(),dim=0)).item()

	
			return eps/count,eps2/count
	# if batch size small enough to fit in one shot
	else:
		with torch.no_grad():
			shape_batch = x.shape[:normalisation_index]
			shape_object = x.shape[normalisation_index:]
			norm_length = np.prod(shape_object)
			#the forward is alittle bit more complex than for linear models as we can't process batched datas in the convolutionnal layers
			# thus we nedd the merge the batch dimension and the sampling dimension before processing
			noise = sample_lp_ball(*(ball_points_number,*shape_batch,norm_length),p=p,device=device,dtype=dtype)

			perturbated_data = (x.view(*shape_batch,norm_length)+ noise*radius).view(*(ball_points_number*np.prod(shape_batch),*shape_object))
			output = model(perturbated_data)
			output = output.view((ball_points_number,shape_batch[0],*output.shape[1:]))
			y_broadcasted = y.unsqueeze(0).expand(ball_points_number,*y.shape)
			return torch.mean(torch.min((torch.argmax(output,dim=2)==y_broadcasted),dim=0).float()).item(),torch.mean(torch.mean((torch.argmax(output,dim=2)==y_broadcasted).float(),dim=0)).item()
#function to compute the confusion matrix of the model on a test set multiple labels
def computing_cm(model,x,y,batch_size):
	model.eval()
	size  = int((torch.max(y)+1).item())

	cm = torch.zeros(size,size,device=x.device)
	if batch_size < x.shape[0]:
		with torch.no_grad():
			for i in range(0,x.shape[0],batch_size):
				output = torch.argmax(model(x[i:i+batch_size]),dim=1)
				truth = y[i:i+batch_size]
				for k,l in zip(output,truth):
					cm[k,l]+=1
			
	else:
		output = torch.argmax(model(x),dim=1)
		for k,l in zip(output,y):
			cm[k,l]+=1
	return cm/torch.sum(cm,dim=1).unsqueeze(1)

def build_adversarial_noise(X,Y,attack_class,**kwargs):
	"""
	Function using an attack too build adversarial noise
	Arguments: X,Y input data
			   attack_class: attack class
	"""
	attack = attack_class(**kwargs)
	x_adv = attack.perturb(X,Y)
	# plt.plot(attack.storage)
	# plt.xlabel("Step")
	# plt.ylabel("Loss")
	# plt.savefig("test.png")
	# exit()
	return x_adv


def compute_norm_gradient(model,X_test,Y_test,criterion):
	output = model(X_test)
	loss = criterion(output,Y_test)
	loss.backward()
	with torch.no_grad():
		sum_grad = sum([torch.sum(torch.abs(param.grad))for param in model.parameters()]).item()
	return sum_grad


def compute_worst_case_accuracy(model,X_test,Y_test,ball_points_number,radius,p=2,device=None,dtype=None):
	dataset = TensorDataset(X_test,Y_test)
	train_loader = DataLoader(dataset,batch_size=25,shuffle=True,num_workers=0)
	results = []
	for data in train_loader:
		x,y = data
		with torch.no_grad():
			noisy_inputs = noise_randomly(ball_points_number,x,radius,p,device=device,dtype=dtype)
			output = model(noisy_inputs)
			inter = (torch.abs((output>0).long()-y.unsqueeze(0).repeat(ball_points_number,*[1 for _ in y.shape]).long())==0.).float()
			pred = torch.min(inter,dim=0)[0]
			acc = torch.sum((pred)).float()/x.shape[0]
			results.append(acc.item())
	return np.mean(results)



def compute_adversarial_accuracy(model,X_test,Y_test,batch_size,attack_cls,kwargs_attack):
	"""
	Function to compute the adversarial accuracy on the test set
	Arguments: model: pt model
			   test_dataset : list of tuples in the form of a pt dataset
			   batch_size: size for the computation of the metric
			   attack class: advertorch class for attack
			   kwargs_attack: kwargs for the attack constructor
	"""
	
	model.eval()
	eps=0.
	size = X_test.shape[0]
	count=0
	
	if batch_size < X_test.shape[0]:
		
		for i in range(0,X_test.shape[0],batch_size):
			count += batch_size
			data = X_test[i:i+batch_size]
			input_model = data
			X_test_adv = build_adversarial_noise(input_model,Y_test[i:i+batch_size],attack_cls,predict=model,**kwargs_attack)
			output = model(X_test_adv)
			eps+= torch.sum(torch.argmax(output,dim=1)==Y_test[i:i+batch_size]).item()
		return eps/count
	else:
		X_test_adv = build_adversarial_noise(X_test,Y_test,attack_cls,predict=model,**kwargs_attack)
		output = model(X_test_adv)
		return torch.mean(torch.argmax(output,dim=1)==Y_test).item()

def robust_loss(model,X_test,Y_test,criterion,batch_size,ball_points_number,radius,p=2,device=None,dtype=None,**kwargs):
	"""
	Training computing the logsumexp computation of the supremum:
	Extra Arguments compared to classic training:
		ball_points_number: number of points to assess the supremum value
		radius: radius of the ball that is sampled
		temperature_generator: scaling factor for the logsumexp operation in the form of a generator, allows some fine tuning of the temperature decay
	This function is not fit for convolutionnal layers, it works only for MLPs. Use compute_robust_accuracy for convolutionnal layers
	"""
	model = nn.DataParallel(model)
	dataset = TensorDataset(X_test,Y_test)
	test_loader = DataLoader(dataset,batch_size=batch_size,shuffle=True,num_workers=0)
	summed=0
	count=0
	for data in test_loader:
		x,y = data
		count += x.shape[0]
		with torch.no_grad():
			noisy_inputs = noise_randomly(ball_points_number,x,radius,p,device=device,dtype=dtype)
			output = model(noisy_inputs)
			batch_nb = x.shape[0]
			loss = criterion(output.view(ball_points_number*batch_nb,output.shape[-1]),y.unsqueeze(0).repeat(ball_points_number,*[1 for _ in y.shape]).view(ball_points_number*batch_nb)).view(ball_points_number,batch_nb)
			loss = torch.sum(torch.max(loss,dim=0)[0])
			summed += loss.item()
	return summed/count

			



def decaying_temperature(initial_value,final_value,decay):
	"""
	Python Generator for the temperature: allows fine tuning of the temperation during the iterations
	here this is a simple linear decreasing temperature
	"""
	t=0
	while True:
		yield initial_value - (initial_value-final_value)*min(1,decay*t)
		t+=1

def load_generic_base(base,dtype,device):
	if base == "avila":
		data = pd.read_csv("data/avila/avila-tr.txt")
		scaler = preprocessing.StandardScaler()
		preprocessor = preprocessing.LabelEncoder()
		X_train = data[["X"+str(i) for i in range(10) if i not in [5,9]]].values
		scaler.fit(X_train)
		X_train = scaler.transform(X_train)
		preprocessor.fit(data['Y'])
		Y_train = preprocessor.transform(data['Y'])
		data = pd.read_csv("data/avila/avila-ts.txt")
		X_test = data[["X"+str(i) for i in range(10) if i not in [5,9]]].values
		Y_test = preprocessor.transform(data['Y'])
		X_test = scaler.transform(X_test)
		X_train,Y_train,X_test,Y_test = torch.Tensor(X_train).to(dtype=dtype,device=device),torch.Tensor(Y_train).to(device=device,dtype=torch.long),torch.Tensor(X_test).to(dtype=dtype,device=device),torch.Tensor(Y_test).to(device=device,dtype=torch.long)
	
	elif base == "grid":
		data = pd.read_csv("data/grid/Data_for_UCI_named.csv")
		X_pd = data[["tau1","tau2","tau3","tau4","p1","p2","p3","p4","g1","g2","g3","g4"]]
		preprocessor = preprocessing.LabelEncoder()
		preprocessor.fit(data['stabf'])
		Y_train = preprocessor.transform(data['stabf'].head(int(len(data)*0.8)))
		Y_test = preprocessor.transform(data['stabf'].tail(int(len(data)*0.2)))
		X_train = X_pd.head(int(len(data)*0.8)).values
		X_test = X_pd.tail(int(len(data)*0.2)).values
		X_train,Y_train,X_test,Y_test = torch.Tensor(X_train).to(dtype=dtype,device=device),torch.Tensor(Y_train).to(device=device,dtype=torch.long),torch.Tensor(X_test).to(dtype=dtype,device=device),torch.Tensor(Y_test).to(device=device,dtype=torch.long)
	elif base == "MNIST":
		train_dataset = torchvision.datasets.MNIST(**{"root":"data", "train":True, "transform":transforms.Compose([transforms.Resize(size=(14,14)),transforms.ToTensor()]), "download":True})
		X_train,Y_train = zip(*train_dataset)
		X_train = torch.stack(X_train,dim=0).to(device=device,dtype=dtype)
		X_train = X_train.reshape(X_train.shape[0],196)
		Y_train = torch.tensor(Y_train).to(device=device)
		test_dataset = torchvision.datasets.MNIST(**{"root":"data", "train":False, "transform":transforms.Compose([transforms.Resize(size=(14,14)),transforms.ToTensor()]), "download":True})
		X_test,Y_test = zip(*test_dataset)
		X_test = torch.stack(X_test,dim=0).to(device=device,dtype=dtype)
		X_test = X_test.reshape(X_test.shape[0],196)
		Y_test = torch.tensor(Y_test).to(device=device)
		
	elif base == "MNISTlatent":
		
		X_train,Y_train = torch.load("data/mnist_latent_train.pt"),torch.load("data/mnist_latent_train_label.pt")
		X_test,Y_test = torch.load("data/mnist_latent_test.pt"),torch.load("data/mnist_latent_test_label.pt")
		X_train = X_train.to(dtype=dtype,device=device)
		Y_train = Y_train.to(device=device,dtype=torch.long)
		X_test = X_test.to(dtype=dtype,device=device)
		Y_test = Y_test.to(device=device,dtype=torch.long)

	else:
		raise NotImplementedError("This set is not implemented yet")

	return X_train,Y_train,X_test,Y_test


def run_function(params,base,model,attack_cls,kwargs_attack,device,dtype,X_train=None,Y_train=None,X_test=None,Y_test=None,**kwargs):

	"""
	Computing the losses
	"""
	if X_train is None or Y_train is None or X_test is None or Y_test is None:

		X_train,Y_train,X_test,Y_test = load_generic_base(base,dtype,device)
	
	ball_points_number=int(params["ball_points_number"])
	radius=max(params["radius"],0.001)


	accuracy = computing_accuracy(model,X_test,Y_test,1000)
	accuracy_robust = compute_robust_accuracy(model,X_test,Y_test,10,ball_points_number,radius,p=kwargs.get("p"),normalisation_index=-1,device=device,dtype=dtype)
	accuracy_adv =compute_adversarial_accuracy(model,X_test,Y_test,10,attack_cls,kwargs_attack)
	return accuracy,accuracy_robust,accuracy_adv

def generate_targeted_adversaries(model,x,y,radius,alpha,steps):
	nb_class = int((torch.max(y)+1).item())
	selection = (y).unsqueeze(0).repeat(nb_class,*[1 for _ in y.shape])
	input_duplicated= x.unsqueeze(0).repeat(nb_class,*[1 for _ in x.shape])
	inpt_0 = input_duplicated.clone()
	faker = (1-y).unsqueeze(0).repeat(nb_class,*[1 for _ in y.shape])

	for step in range(steps):

		input_duplicated.requires_grad = True

		output = model(input_duplicated)

		loss = output[faker.long()]-output[selection.long()]
		loss = torch.sum(loss)
		loss.backward()
		with torch.no_grad():
			perturbation = input_duplicated -inpt_0 + alpha*input_duplicated.grad
			input_duplicated = torch.clamp(perturbation,-radius,radius) + inpt_0
	return input_duplicated


def compute_spectral_norm(weight,n_power_iterations,do_power_iteration=True,eps=1e-12):
	weight_mat = weight
	h, w = weight_mat.size()
	# randomly initialize `u` and `v`
	u = F.normalize(weight.new_empty(h).normal_(0, 1), dim=0, eps=eps)
	v = F.normalize(weight.new_empty(w).normal_(0, 1), dim=0, eps=eps)
	if do_power_iteration:
			with torch.no_grad():
				for _ in range(n_power_iterations):
					# Spectral norm of weight equals to `u^T W v`, where `u` and `v`
					# are the first left and right singular vectors.
					# This power iteration produces approximations of `u` and `v`.
					v = F.normalize(torch.mv(weight_mat.t(), u), dim=0, eps=eps, out=v)
					u = F.normalize(torch.mv(weight_mat, v), dim=0, eps=eps, out=u)
				if n_power_iterations > 0:
					# See above on why we need to clone
					u = u.clone()
					v = v.clone()

	sigma = torch.dot(u, torch.mv(weight_mat, v))
	return sigma



def plot_pcolor(X_test,Z_test,X_train,Z_train,model,N,title,name,device,dtype):
	"""
	Plotting the 2D landscape using testing, training data and the model
	"""
	plt.figure(figsize=(20,10))
	xmin,ymin = torch.min(X_train,dim=0)[0].cpu().numpy()
	xmax,ymax = torch.max(X_train,dim=0)[0].cpu().numpy()
	x,y = np.meshgrid(np.linspace(xmin,xmax,N),
					np.linspace(ymin,ymax,N))
	with torch.no_grad():
		X_grid= torch.tensor(np.stack([x,y],axis=-1)).to(device=device,dtype=dtype)
		Z_pt = torch.sigmoid(model(X_grid).squeeze())
		Z_mesh = Z_pt.cpu().numpy()
	
	plt.contourf(x,y,Z_mesh,levels=[0.,0.001,0.01,0.1,0.2,0.3,0.4,0.45,0.50,0.55,0.6,0.7,0.8,0.9,0.99,0.999,1.],alpha=0.7)
	fig = plt.contour(x,y,Z_mesh,levels=[0.,0.001,0.01,0.1,0.2,0.3,0.4,0.45,0.50,0.55,0.6,0.7,0.8,0.9,0.99,0.999,1.],colors='k')
	plt.clabel(fig, fontsize=9, inline=1)
	#plt.scatter(X_train[:,0].cpu().numpy(),X_train[:,1].cpu().numpy(),c=Z_train.squeeze().cpu().numpy(),edgecolors='black',alpha=0.1)
	plt.scatter(X_test[:,0].cpu().numpy(),X_test[:,1].cpu().numpy(),c=Z_test.squeeze().cpu().numpy(),edgecolors='black')
	plt.title(title)
	plt.xlim(xmin,xmax)
	plt.ylim(ymin,ymax)


	plt.savefig(name+".png")


def n_exponential(epsilon):
	return int(616*(100**(epsilon/0.24))) #0.55 instead of 0.24 on 2D toy bases



def evaluate_metrics_on_bases():
	"""
	Function to evaluation the different losses 
	"""
	

	device = torch.device("cuda")
	dtype = torch.float



	number_of_test=51
	
	eps = [0.3,0.25,0.2,0.15,0.10,0.05,0.01]


	label_save_file="avila_models.pk"
	model_folder = "models/"
	logs={}
	for radius in tqdm(eps):
		results = np.zeros((number_of_test,4))
		for i in tqdm(range(number_of_test),leave=False):
			
			
			
			# activation = nn.ReLU()
			# activation = nn.ELU()
			activation = GeneralizedRelu.apply
			model = MLPModel(output_classes=12,non_linearity=activation)
			model.load_state_dict(torch.load(model_folder+"eps_"+ str(radius)+"_trial_"+str(i)+".pt"))
			model.to(device=device,dtype=dtype)
			deactivate(model)

			kwargs_pgd = {
			"loss_fn":nn.CrossEntropyLoss(),
			"eps":radius,
			"nb_iter":30,
			"eps_iter":0.02,
			"clip_min":-100000,
			"clip_max":1000000}

			kwargs={
			"p":np.inf,
			"device":device,
			"dtype":dtype,
			"model":model,
			"base":"avila",
			"attack_cls":LinfPGDAttackNew,
			"kwargs_attack":kwargs_pgd,
			}


			args={"ball_points_number":n_exponential(radius),
					"radius":radius,
					"final_temperature":0.0001}


			accuracy,accuracy_robust,accuracy_adv= run_function(args,**kwargs)
			results[i,0]=accuracy
			results[i,1]=accuracy_robust[0]
			results[i,2]=accuracy_adv
			results[i,3]=accuracy_robust[1]

		logs[radius]=results
		with open(label_save_file, 'wb') as pk_file:
			pickle.dump(logs, pk_file, protocol=pickle.HIGHEST_PROTOCOL)

	
def evaluate_lipschitz_constant():
	device = torch.device("cuda")
	dtype = torch.float

	number_of_test=51
	
	eps = [0.3,0.25,0.2,0.15,0.1,0.05,0.01]

	# print("gen relu,robust,many epochs")
	# X_train,Y_train,X_test,Y_test = select_base("saw",dtype,device)
	label_save_file="lipschitz_avila.pk"
	model_folder = "models/"
	logs={}
	for radius in tqdm(eps):
		results = np.zeros((number_of_test))
		for i in tqdm(range(number_of_test),leave=False):
			
			
			
			# activation = nn.ReLU()
			activation = nn.ELU()
			model = MLPModel(output_classes=12,non_linearity=activation)
			model.load_state_dict(torch.load(model_folder+"eps_"+ str(radius)+"_trial_"+str(i)+".pt"))
			model.to(device=device,dtype=dtype)
			deactivate(model)
			list_layers = [model.layer1,nn.ReLU(),model.layer2 ,nn.ReLU(), model.layer3]
			results[i] = run_lipschitz_algorithm(deepcopy(list_layers),nb_individuals=100,NGEN=3200,cxpb=0.5, mutpb=0.2,tournsize=3,indpb=0.02,dtype=dtype,device=device)
		logs[radius]=results
	with open(label_save_file, 'wb') as pk_file:
		pickle.dump(logs, pk_file, protocol=pickle.HIGHEST_PROTOCOL)



if __name__ == "__main__":
	device = torch.device("cuda")
	dtype = torch.float
	evaluate_metrics_on_bases()
	evaluate_lipschitz_constant()
